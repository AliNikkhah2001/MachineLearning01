# -*- coding: utf-8 -*-
"""Q1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1db80KdIbU55i5mP-QJnL6AEk8VZE0yBj

<h1 align="center">Introduction to Machine Learning - 25737-2</h1>
<h4 align="center">Dr. R. Amiri</h4>
<h4 align="center">Sharif University of Technology, Spring 2024</h4>


<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Titillium+Web:ital,wght@0,200;0,300;0,400;0,600;0,700;0,900;1,200;1,300;1,400;1,600;1,700&display=swap" rel="stylesheet">
<div class="box" style="padding: 10px; margin: 10px 0; background-color: gray; color: white; border-radius: 5px; font-size: 15px;">
  <table style="padding: 10px; margin: auto auto; background-color: gray;  border-radius: 5px; font-size: 15px;">
      <tr>
          <th colspan='2'><h1 style="text-align: center">
Machine Learning </br>
</h1>
<h2 style="text-align: center">
Course Assignment Two </br>
</h2>
</th>
<tr>
    <tr>
      <th colspan="2">Personal Info</th>
    </tr>
    <tr>
      <td>First Name:</td>
      <td>Ali</td>
    </tr>
    <tr>
      <td>Last Name:</td>
      <td>Nikkhah</td>
    </tr>
    <tr>
      <td>Student Number:</td>
      <td>99102445</td>
    </tr>
    <tr>
      <td>Github:</td>
      <td><a href="https://github.com/AliNikkhah2001/MachineLearning02" target="_blank">https://github.com/AliNikkhah2001/MachineLearning02</a></td>
    </tr>
  </table>
</div>

### **Briefing:**
 **In this section, we are going to investigate linear regression and its extensions ridge and lasso shrinkage methods in Boston dataset.**
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Lasso
from sklearn.metrics import r2_score, mean_squared_error
import matplotlib.pyplot as plt
from sklearn.linear_model import Ridge
import numpy as np

"""## **Part 1**
First familiarize yourself with Boston dataset. Briefly explain its features and dataset's shape. You are provided with the required dataset named **boston.csv**.

we start by first printing description of the dataset, then we show illustrative plots that lead to normalization of the dataset.
"""

# Load the dataset
boston_df = pd.read_csv("Q1/Boston.csv")

# Display the first few rows of the dataset to understand its structure
print(boston_df.head())

""" The Boston Housing Dataset description

The Boston Housing Dataset is a derived from information collected by the U.S. Census Service concerning housing in the area of Boston MA. The following describes the dataset columns:

- CRIM - per capita crime rate by town
- ZN - proportion of residential land zoned for lots over 25,000 sq.ft.
- INDUS - proportion of non-retail business acres per town.
- CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)
- NOX - nitric oxides concentration (parts per 10 million)
- RM - average number of rooms per dwelling
- AGE - proportion of owner-occupied units built prior to 1940
- DIS - weighted distances to five Boston employment centres
- RAD - index of accessibility to radial highways
- TAX - full-value property-tax rate per 10,000 dollar
- PTRATIO - pupil-teacher ratio by town
- B  1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town
- LSTAT -  lower status of the population
- MEDV - Median value of owner-occupied homes in $1000's
"""

print(boston_df.describe())

import seaborn as sns
import matplotlib.pyplot as plt
from scipy import stats

fig, axs = plt.subplots(ncols=7, nrows=2, figsize=(20, 10))
index = 0
axs = axs.flatten()
for k,v in boston_df.items():
    sns.boxplot(y=k, data=boston_df, ax=axs[index])
    index += 1
plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)

fig, axs = plt.subplots(ncols=7, nrows=2, figsize=(20, 10))
index = 0
axs = axs.flatten()
for k,v in boston_df.items():
    sns.histplot(v, ax=axs[index], kde=True, stat="density")
    index += 1
plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)

from sklearn import preprocessing
# Let's scale the columns before plotting them against MEDV
min_max_scaler = preprocessing.MinMaxScaler()
column_sels = ['LSTAT', 'INDUS', 'PTRATIO', 'RM', 'TAX', 'DIS', 'AGE','NX']
x = boston_df.loc[:,column_sels]
y = boston_df['MEDV']
x = pd.DataFrame(data=min_max_scaler.fit_transform(x), columns=column_sels)
fig, axs = plt.subplots(ncols=4, nrows=2, figsize=(20, 10))
index = 0
axs = axs.flatten()
for i, k in enumerate(column_sels):
    sns.regplot(y=y, x=x[k], ax=axs[i])
plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)

"""## **Part 2**
Split dataset into train and test sets. Train linear regression model using all of the features. Report $R^2$ and RMSE error metrics for both train and test sets. Also report all of the coefficients.
"""

# Split the dataset into train and test sets
X = boston_df.drop(columns=['MEDV'])  # Features
y = boston_df['MEDV']  # Target variable
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the linear regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Predict on training set
train_pred = model.predict(X_train)

# Predict on test set
test_pred = model.predict(X_test)

# Calculate R^2 scores
train_r2 = r2_score(y_train, train_pred)
test_r2 = r2_score(y_test, test_pred)

# Calculate RMSE
train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))
test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))

# Get coefficients
coefficients = model.coef_

print("Train R^2 score:", train_r2)
print("Test R^2 score:", test_r2)
print("Train RMSE:", train_rmse)
print("Test RMSE:", test_rmse)
print("Coefficients:", coefficients)

"""| Metric            | Train            | Test             |
|-------------------|------------------|------------------|
| R^2 Score         | 0.7509           | 0.6688           |
| RMSE              | 4.6520           | 4.9286           |

Formulas:
- R^2 Score: $$ R^2 = 1 - \frac{SS_{res}}{SS_{tot}} $$
- RMSE: $$ RMSE = \sqrt{\frac{1}{n}\sum_{i=1}^{n}(y_{true} - y_{pred})^2}$$

## **Part 3**
Now, we want to see the effect of ridge regression on learning process. To do so, set the alphas parameter as follows for synchronization:$$ alphas = 10**np.linspace (3,-3,100) *0.5$$

$$ \text{minimize} \left( \sqrt{\frac{1}{n}\sum_{i=1}^{n}(y_{true} - y_{pred})^2} + \alpha \sum_{j=1}^{p} \omega_j^2 \right)$$

Now, plot the following in separate plots:

1- MSE for train and test versus α

2- Coefficients versus α

3- Number of not-considered features versus α

Finally select the optimal value of α. Report MSE, $R^2$ and coefficients.Compare this case with the linear regression case.
"""

# Define alphas
alphas = 10 ** np.linspace(3, -3, 100)*0.5

# Initialize lists to store results
train_mse = []
test_mse = []
coefficients = []
not_considered_features = []

for alpha in alphas:
    # Train ridge regression model
    ridge_model = Ridge(alpha=alpha)
    ridge_model.fit(X_train, y_train)

    # Predict on training set
    train_pred = ridge_model.predict(X_train)

    # Predict on test set
    test_pred = ridge_model.predict(X_test)

    # Calculate MSE for training and test sets
    train_mse.append(mean_squared_error(y_train, train_pred))
    test_mse.append(mean_squared_error(y_test, test_pred))

    # Store coefficients
    coefficients.append(ridge_model.coef_)

    # Count number of features with coefficient equal to zero
    not_considered_features.append(np.sum(ridge_model.coef_ == 0))

# Find the alpha where the derivative equals zero
optimal_alpha = alphas[np.argmin(np.abs(test_mse))]

# Plot MSE for train and test versus α
plt.figure(figsize=(10, 5))
plt.semilogx(alphas, train_mse, label='Train MSE')
plt.semilogx(alphas, test_mse, label='Test MSE')
plt.xlabel('Log Alpha')
plt.ylabel('MSE')
plt.title('MSE for Train and Test vs. Log Alpha')
plt.legend()

# Plot the horizontal dashed line at the lowest point of the training MSE
plt.axvline(x=optimal_alpha, color='gray', linestyle='--', linewidth=1)
plt.text(optimal_alpha, min(train_mse), f'Optimal Alpha: {optimal_alpha:.4f}',
         horizontalalignment='right')

plt.show()

# Plot coefficients versus α
coefficients = np.array(coefficients)
plt.figure(figsize=(10, 5))
for i in range(coefficients.shape[1]):
    plt.plot(alphas, coefficients[:, i], label=f'Feature {i}')
plt.xlabel('Log Alpha')
plt.ylabel('Coefficient Value')
plt.title('Coefficients vs. Log Alpha')
plt.legend()
plt.show()

# Plot Number of not-considered features versus α
plt.figure(figsize=(10, 5))
plt.plot(alphas, not_considered_features)
plt.xlabel('Log Alpha')
plt.ylabel('Number of Not-Considered Features')
plt.title('Number of Not-Considered Features vs. Log Alpha')
plt.show()

# Report results
print("Optimal Alpha (Derivative Zero):", optimal_alpha)
print("Optimal Alpha (Train MSE) :", alphas[np.argmin(train_mse)])
print("Optimal Alpha (Test MSE) :", alphas[np.argmin(test_mse)])

# Initialize lists to store R^2 values
train_r2 = []
test_r2 = []

for alpha in alphas:
    # Train ridge regression model
    ridge_model = Ridge(alpha=alpha)
    ridge_model.fit(X_train, y_train)

    # Predict on training set
    train_pred = ridge_model.predict(X_train)

    # Predict on test set
    test_pred = ridge_model.predict(X_test)

    # Calculate R^2 for training and test sets
    train_r2.append(ridge_model.score(X_train, y_train))
    test_r2.append(ridge_model.score(X_test, y_test))

# Find the index of the optimal alpha where the test MSE is minimized
optimal_alpha_idx = np.argmin(test_mse)

# Find the corresponding R^2 value for the optimal alpha
optimal_alpha_r2 = test_r2[optimal_alpha_idx]

# Plot R^2 for train and test versus α
plt.figure(figsize=(10, 5))
plt.semilogx(alphas, train_r2, label='Train R^2')
plt.semilogx(alphas, test_r2, label='Test R^2')
plt.xlabel('Log Alpha')
plt.ylabel('R^2')
plt.title('R^2 for Train and Test vs. Log Alpha')
plt.legend()

# Plot the optimal alpha point on the curve
plt.scatter(alphas[optimal_alpha_idx], optimal_alpha_r2, color='red', label=f'Optimal Alpha ({alphas[optimal_alpha_idx]:.4f}, {optimal_alpha_r2:.4f})')
plt.text(alphas[optimal_alpha_idx], optimal_alpha_r2, f'({alphas[optimal_alpha_idx]:.4f}, {optimal_alpha_r2:.4f})',
         horizontalalignment='right', verticalalignment='bottom')

plt.show()

# Report results
print("Optimal Alpha (Test MSE Minimized):", alphas[optimal_alpha_idx])
print("Optimal Alpha R^2:", optimal_alpha_r2)

"""## Explanation:

as we could have guessed earlier,Ridge regression squared term of parameters tends to make absolute value of parameters smaller, but it doesn't set any parameter value to zero during training.


![lasso regression](https://miro.medium.com/v2/resize:fit:761/1*nrWncnoJ4V_BkzEf1pd4MA.png)

Ridge regression is a type of linear regression that introduces a regularization term to the loss function, in order to prevent overfitting by imposing a penalty on the size of the coefficients. Mathematically, the loss function for Ridge regression can be written as:

$$
L_{\text{Ridge}} = \text{MSE} + \lambda \sum_{j=1}^{p} \theta_j^2
$$

The regularization term is added to the ordinary least squares loss function. This term penalizes the model for having large coefficients, effectively shrinking them towards zero.

Ridge regression works by finding the coefficients that minimize the combined loss function. By adjusting the values of the coefficients, Ridge regression finds a balance between fitting the training data well and keeping the coefficients small to avoid overfitting.

However, even though Ridge regression penalizes the size of the coefficients, it does not force them to reach exactly zero unless $\lambda$ is extremely large. This is because the penalty term $\lambda \sum_{j=1}^{p} \theta_j^2$ is continuous and smooth, allowing the coefficients to be reduced but not completely eliminated. As \( \lambda \) increases, the coefficients tend to become smaller, but they will never be exactly zero in Ridge regression unless $\lambda$ approaches infinity.

This property of Ridge regression makes it suitable for situations where all features are relevant to the prediction task, as it will not completely discard any feature, but rather reduce their impact on the model according to their importance in predicting the target variable.

## **Part 4**
Repeat Part 3 with lasso regression. Where do you think we should consider using lasso regression?
"""

from sklearn.linear_model import Lasso

# Define alphas
alphas = 10 ** np.linspace(3, -3, 100) * 0.5

# Initialize lists to store results
train_mse = []
test_mse = []
coefficients = []
not_considered_features = []

for alpha in alphas:
    # Train Lasso regression model
    lasso_model = Lasso(alpha=alpha)
    lasso_model.fit(X_train, y_train)

    # Predict on training set
    train_pred = lasso_model.predict(X_train)

    # Predict on test set
    test_pred = lasso_model.predict(X_test)

    # Calculate MSE for training and test sets
    train_mse.append(mean_squared_error(y_train, train_pred))
    test_mse.append(mean_squared_error(y_test, test_pred))

    # Store coefficients
    coefficients.append(lasso_model.coef_)

    # Count number of features with coefficient equal to zero
    not_considered_features.append(np.sum(lasso_model.coef_ == 0))

# Find the alpha where the test MSE is minimized
optimal_alpha = alphas[np.argmin(test_mse)]

# Plot MSE for train and test versus α
plt.figure(figsize=(10, 5))
plt.semilogx(alphas, train_mse, label='Train MSE')
plt.semilogx(alphas, test_mse, label='Test MSE')
plt.xlabel('Log Alpha')
plt.ylabel('MSE')
plt.title('MSE for Train and Test vs. Log Alpha')
plt.legend()

# Plot the horizontal dashed line at the lowest point of the training MSE
plt.axvline(x=optimal_alpha, color='gray', linestyle='--', linewidth=1)
plt.text(optimal_alpha, min(train_mse), f'Optimal Alpha: {optimal_alpha:.4f}',
         horizontalalignment='right')

plt.show()

# Plot coefficients versus α
coefficients = np.array(coefficients)
plt.figure(figsize=(10, 5))
for i in range(coefficients.shape[1]):
    plt.plot(alphas, coefficients[:, i], label=f'Feature {i}')
plt.xlabel('Log Alpha')
plt.ylabel('Coefficient Value')
plt.title('Coefficients vs. Log Alpha')
plt.legend()
plt.show()

# Plot Number of not-considered features versus α
plt.figure(figsize=(10, 5))
plt.plot(alphas, not_considered_features)
plt.xlabel('Log Alpha')
plt.ylabel('Number of Not-Considered Features')
plt.title('Number of Not-Considered Features vs. Log Alpha')
plt.show()

# Report results
print("Optimal Alpha (Test MSE Minimized):", optimal_alpha)
print("Optimal Alpha Train MSE:", alphas[np.argmin(train_mse)])
print("Optimal Alpha Test MSE:", alphas[np.argmin(test_mse)])

from sklearn.linear_model import Lasso

# Initialize lists to store R^2 values
train_r2 = []
test_r2 = []

for alpha in alphas:
    # Train Lasso regression model
    lasso_model = Lasso(alpha=alpha)
    lasso_model.fit(X_train, y_train)

    # Predict on training set
    train_pred = lasso_model.predict(X_train)

    # Predict on test set
    test_pred = lasso_model.predict(X_test)

    # Calculate R^2 for training and test sets
    train_r2.append(lasso_model.score(X_train, y_train))
    test_r2.append(lasso_model.score(X_test, y_test))

# Find the index of the optimal alpha where the test MSE is minimized
optimal_alpha_idx = np.argmin(test_mse)

# Find the corresponding R^2 value for the optimal alpha
optimal_alpha_r2 = test_r2[optimal_alpha_idx]

# Plot R^2 for train and test versus α
plt.figure(figsize=(10, 5))
plt.semilogx(alphas, train_r2, label='Train R^2')
plt.semilogx(alphas, test_r2, label='Test R^2')
plt.xlabel('Log Alpha')
plt.ylabel('R^2')
plt.title('R^2 for Train and Test vs. Log Alpha')
plt.legend()

# Plot the optimal alpha point on the curve
plt.scatter(alphas[optimal_alpha_idx], optimal_alpha_r2, color='red', label=f'Optimal Alpha ({alphas[optimal_alpha_idx]:.4f}, {optimal_alpha_r2:.4f})')
plt.text(alphas[optimal_alpha_idx], optimal_alpha_r2, f'({alphas[optimal_alpha_idx]:.4f}, {optimal_alpha_r2:.4f})',
         horizontalalignment='right', verticalalignment='bottom')

plt.show()

# Report results
print("Optimal Alpha (Test MSE Minimized):", alphas[optimal_alpha_idx])
print("Optimal Alpha R^2:", optimal_alpha_r2)

"""### As we saw, big values of alpha will lead to early set of all the coefficients to zero, so we shorthen the higher band for alpha values for a better model training and representation of the training process."""

from sklearn.linear_model import Lasso

# Define alphas
alphas = 10 ** np.linspace(1, -3, 100) * 0.5

# Initialize lists to store results
train_mse = []
test_mse = []
coefficients = []
not_considered_features = []

for alpha in alphas:
    # Train Lasso regression model
    lasso_model = Lasso(alpha=alpha)
    lasso_model.fit(X_train, y_train)

    # Predict on training set
    train_pred = lasso_model.predict(X_train)

    # Predict on test set
    test_pred = lasso_model.predict(X_test)

    # Calculate MSE for training and test sets
    train_mse.append(mean_squared_error(y_train, train_pred))
    test_mse.append(mean_squared_error(y_test, test_pred))

    # Store coefficients
    coefficients.append(lasso_model.coef_)

    # Count number of features with coefficient equal to zero
    not_considered_features.append(np.sum(lasso_model.coef_ == 0))

# Find the alpha where the test MSE is minimized
optimal_alpha = alphas[np.argmin(test_mse)]

# Plot MSE for train and test versus α
plt.figure(figsize=(10, 5))
plt.semilogx(alphas, train_mse, label='Train MSE')
plt.semilogx(alphas, test_mse, label='Test MSE')
plt.xlabel('Log Alpha')
plt.ylabel('MSE')
plt.title('MSE for Train and Test vs. Log Alpha')
plt.legend()

# Plot the horizontal dashed line at the lowest point of the training MSE
plt.axvline(x=optimal_alpha, color='gray', linestyle='--', linewidth=1)
plt.text(optimal_alpha, min(train_mse), f'Optimal Alpha: {optimal_alpha:.4f}',
         horizontalalignment='right')

plt.show()

# Plot coefficients versus α
coefficients = np.array(coefficients)
plt.figure(figsize=(10, 5))
for i in range(coefficients.shape[1]):
    plt.plot(alphas, coefficients[:, i], label=f'Feature {i}')
plt.xlabel('Log Alpha')
plt.ylabel('Coefficient Value')
plt.title('Coefficients vs. Log Alpha')
plt.legend()
plt.show()

# Plot Number of not-considered features versus α
plt.figure(figsize=(10, 5))
plt.plot(alphas, not_considered_features)
plt.xlabel('Log Alpha')
plt.ylabel('Number of Not-Considered Features')
plt.title('Number of Not-Considered Features vs. Log Alpha')
plt.show()

"""# Explanation
![lasso regression](https://miro.medium.com/v2/resize:fit:761/1*nrWncnoJ4V_BkzEf1pd4MA.png)
Lasso regression, like Ridge regression, is a type of linear regression that adds a regularization term to the loss function. However, Lasso regression uses the \( L_1 \) norm of the coefficients as the regularization term, whereas Ridge regression uses the \( L_2 \) norm. Mathematically, the loss function for Lasso regression can be written as:

$$
L_{\text{Lasso}} = \text{MSE} + \lambda \sum_{j=1}^{p} |\theta_j|
$$


The regularization term $\lambda \sum_{j=1}^{p} |\theta_j|$ is added to the ordinary least squares loss function. This term encourages sparsity in the coefficients by penalizing large coefficient values.

Lasso regression works by finding the coefficients  $\theta$ that minimize the combined loss function. Due to the $L_1$ regularization term, Lasso regression tends to produce sparse solutions, where many of the coefficients are exactly zero. This means that Lasso regression can effectively perform feature selection by eliminating irrelevant features from the model.

The reason parameters can reach exactly zero in Lasso regression is due to the shape of the $L_1$ norm. Unlike the $L_2$ norm used in Ridge regression, the $L_1$ norm has sharp corners at the axes. This sharpness at the corners encourages some coefficients to be exactly zero when the regularization parameter $\lambda$ is sufficiently large. As $\lambda$ increases, the penalty for having non-zero coefficients becomes stronger, leading more coefficients to be driven to exactly zero.

This property of Lasso regression makes it particularly useful for situations where feature selection is important, as it can automatically discard irrelevant features from the model by setting their coefficients to zero.

## **Part 5**
In this part, we intend to see the effect of ridge and lasso regression methods in overfitting prevention. In order to do this, change the ratio of training data to the whole data from low to high. As you know, the lower the ratio of trainig data, the more likely the overfitting would be. for each ratio, fit linear regression, ridge regression and lasso regression to data and then plot the following:

1- $R^2$ for test data versus ratio of training data (Three methods in one plot)

2- Selected α value versus training data ratio for ridge and lasso regression methods (In one plot)

How is the overall behavior of these plots and how do you analyze them?
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.metrics import r2_score
# Define different ratios of training data
ratios = np.linspace(0.1, 0.95, 300)

# Initialize lists to store results
linear_regression_rms = []
ridge_optimal_alphas = []
lasso_optimal_alphas = []
ridge_rms = []
lasso_rms = []

# Define alphas
alphas = 10 ** np.linspace(3, -3, 100) * 0.5
# Iterate over different ratios
for ratio in ratios:
    # Split the data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1 - ratio, random_state=42)

    # Linear Regression
    linear_model = LinearRegression()
    linear_model.fit(X_train, y_train)
    linear_pred = linear_model.predict(X_test)
    linear_regression_rms.append(mean_squared_error(y_test, linear_pred))

    # Ridge Regression
    ridge_train_rms = []
    for alpha in alphas:
        ridge_model = Ridge(alpha=alpha)
        ridge_model.fit(X_train, y_train)
        ridge_pred = ridge_model.predict(X_test)
        ridge_train_rms.append(mean_squared_error(y_test, ridge_pred))
    optimal_alpha_idx = np.argmin(ridge_train_rms)
    ridge_optimal_alphas.append(alphas[optimal_alpha_idx])
    ridge_rms.append(ridge_train_rms[optimal_alpha_idx])

    # Lasso Regression
    lasso_train_rms = []
    for alpha in alphas:
        lasso_model = Lasso(alpha=alpha)
        lasso_model.fit(X_train, y_train)
        lasso_pred = lasso_model.predict(X_test)
        lasso_train_rms.append(mean_squared_error(y_test, lasso_pred))
    optimal_alpha_idx = np.argmin(lasso_train_rms)
    lasso_optimal_alphas.append(alphas[optimal_alpha_idx])
    lasso_rms.append(lasso_train_rms[optimal_alpha_idx])

# Plot R^2 for test data versus ratio of training data
plt.figure(figsize=(10, 5))
plt.plot(ratios, linear_regression_rms, label='Linear Regression')
plt.plot(ratios, ridge_rms, label='Ridge Regression (Optimal Alpha)')
plt.plot(ratios, lasso_rms, label='Lasso Regression (Optimal Alpha)')
plt.xlabel('Ratio of Training Data')
plt.ylabel('RMS for Test Data')
plt.title('RMS for Test Data vs. Ratio of Training Data')
plt.legend()
plt.show()

# Plot R^2 for test data versus ratio of training data
plt.figure(figsize=(10, 5))
plt.plot(ratios, ridge_rms, label='Ridge Regression (Optimal Alpha)')
plt.plot(ratios, lasso_rms, label='Lasso Regression (Optimal Alpha)')
plt.xlabel('Ratio of Training Data')
plt.ylabel('RMS for Test Data')
plt.title('RMS for Test Data vs. Ratio of Training Data')
plt.legend()
plt.show()

# Plot R^2 for test data versus ratio of training data
plt.figure(figsize=(10, 5))
plt.plot(ratios, linear_regression_rms, label='Linear Regression')
plt.xlabel('Ratio of Training Data')
plt.ylabel('RMS for Test Data')
plt.title('RMS for Test Data vs. Ratio of Training Data')
plt.legend()
plt.show()

# Plot selected alpha value versus training data ratio for Ridge and Lasso Regression
plt.figure(figsize=(10, 5))
plt.plot(ratios, ridge_optimal_alphas, label='Optimal Alpha for Ridge')
plt.xlabel('Ratio of Training Data')
plt.ylabel('Selected Alpha Value')
plt.title('Selected Alpha Value vs. Ratio of Training Data')
plt.legend()
plt.show()

# Plot selected alpha value versus training data ratio for Ridge and Lasso Regression
plt.figure(figsize=(10, 5))
plt.plot(ratios, lasso_optimal_alphas, label='Optimal Alpha Lasso Regression ')
plt.xlabel('Ratio of Training Data')
plt.ylabel('Selected Alpha Value')
plt.title('Selected Alpha Value vs. Ratio of Training Data')
plt.legend()
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.metrics import r2_score

# Define different ratios of training data
ratios = np.linspace(0.1, 0.95, 300)

# Initialize lists to store results
linear_regression_r2 = []
ridge_optimal_alphas = []
lasso_optimal_alphas = []
ridge_optimal_r2=[]
lasso_optimal_r2=[]

# Iterate over different ratios
for ratio in ratios:
    # Split the data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1 - ratio, random_state=42)

    # Linear Regression
    linear_model = LinearRegression()
    linear_model.fit(X_train, y_train)
    linear_pred = linear_model.predict(X_test)
    linear_regression_r2.append(r2_score(y_test, linear_pred))

    # Ridge Regression
    ridge_train_r2 = []
    for alpha in alphas:
        ridge_model = Ridge(alpha=alpha)
        ridge_model.fit(X_train, y_train)
        ridge_pred = ridge_model.predict(X_test)
        ridge_train_r2.append(r2_score(y_test, ridge_pred))
    optimal_alpha_idx = np.argmax(ridge_train_r2)  # Use argmax for R2 metric
    ridge_optimal_alphas.append(alphas[optimal_alpha_idx])
    ridge_optimal_r2.append(ridge_train_r2[optimal_alpha_idx])

    # Lasso Regression
    lasso_train_r2 = []
    for alpha in alphas:
        lasso_model = Lasso(alpha=alpha)
        lasso_model.fit(X_train, y_train)
        lasso_pred = lasso_model.predict(X_test)
        lasso_train_r2.append(r2_score(y_test, lasso_pred))
    optimal_alpha_idx = np.argmax(lasso_train_r2)  # Use argmax for R2 metric
    lasso_optimal_alphas.append(alphas[optimal_alpha_idx])
    lasso_optimal_r2.append(lasso_train_r2[optimal_alpha_idx])


# Plot R2 for test data versus ratio of training data
plt.figure(figsize=(10, 5))
plt.plot(ratios, linear_regression_r2, label='Linear Regression')
plt.plot(ratios, ridge_optimal_r2, label='Ridge Regression (Optimal R2)')
plt.plot(ratios, lasso_optimal_r2, label='Lasso Regression (Optimal R2)')
plt.xlabel('Ratio of Training Data')
plt.ylabel('R2 for Test Data')
plt.title('R2 for Test Data vs. Ratio of Training Data')
plt.legend()
plt.show()

# Plot selected alpha value versus training data ratio for Ridge Regression
plt.figure(figsize=(10, 5))
plt.plot(ratios, ridge_optimal_alphas, label='Optimal Alpha for Ridge')
plt.xlabel('Ratio of Training Data')
plt.ylabel('Selected Alpha Value')
plt.title('Selected Alpha Value for Ridge Regression vs. Ratio of Training Data')
plt.legend()
plt.show()

# Plot selected alpha value versus training data ratio for Lasso Regression
plt.figure(figsize=(10, 5))
plt.plot(ratios, lasso_optimal_alphas, label='Optimal Alpha for Lasso')
plt.xlabel('Ratio of Training Data')
plt.ylabel('Selected Alpha Value')
plt.title('Selected Alpha Value for Lasso Regression vs. Ratio of Training Data')
plt.legend()
plt.show()

"""### Results

$$
\textbf{Impact of Lasso and Ridge Regression on Overfitting}
$$

$$
\textbf{Ridge Regression:}
$$

Ridge regression introduces a penalty term to the loss function that penalizes the size of the coefficients. This penalty encourages the model to shrink the coefficients towards zero, effectively reducing the complexity of the model. By reducing the magnitude of the coefficients, Ridge regression helps prevent overfitting by limiting the model's ability to fit noise in the data. However, Ridge regression does not force coefficients to be exactly zero, allowing all features to contribute to the prediction to some extent. This makes Ridge regression particularly useful when all features are relevant to the prediction task but might still be prone to overfitting due to multicollinearity or high dimensionality.

$$
\textbf{Lasso Regression:}
$$

Lasso regression, similar to Ridge regression, adds a penalty term to the loss function to prevent overfitting. However, Lasso regression uses the $L_1$ norm of the coefficients as the penalty term, which has the effect of sparsity-inducing regularization. This means that Lasso regression tends to produce sparse solutions where many coefficients are exactly zero. By setting some coefficients to zero, Lasso regression performs feature selection and effectively eliminates irrelevant features from the model. This not only reduces the complexity of the model but also improves interpretability by identifying the most important features for prediction. Lasso regression is especially beneficial when dealing with high-dimensional datasets with many irrelevant features, as it helps mitigate the risk of overfitting by selecting only the most relevant features for the task.

In summary, both Ridge and Lasso regression are effective techniques for mitigating overfitting in regression models. Ridge regression reduces the magnitude of coefficients, while Lasso regression performs feature selection by setting some coefficients to zero. These regularization techniques help strike a balance between bias and variance, ultimately improving the generalization performance of the model on unseen data.
"""

