{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I8eZGMPb-GlP"
   },
   "source": [
    "<h1 align=\"center\">Introduction to Machine Learning - 25737-2</h1>\n",
    "<h4 align=\"center\">Dr. R. Amiri</h4>\n",
    "<h4 align=\"center\">Sharif University of Technology, Spring 2024</h4>\n",
    "\n",
    "\n",
    "<link rel=\"preconnect\" href=\"https://fonts.googleapis.com\">\n",
    "<link rel=\"preconnect\" href=\"https://fonts.gstatic.com\" crossorigin>\n",
    "<link href=\"https://fonts.googleapis.com/css2?family=Titillium+Web:ital,wght@0,200;0,300;0,400;0,600;0,700;0,900;1,200;1,300;1,400;1,600;1,700&display=swap\" rel=\"stylesheet\">\n",
    "<div class=\"box\" style=\"padding: 10px; margin: 10px 0; background-color: gray; color: white; border-radius: 5px; font-size: 15px;\">\n",
    "  <table style=\"padding: 10px; margin: auto auto; background-color: gray;  border-radius: 5px; font-size: 15px;\">\n",
    "      <tr>\n",
    "          <th colspan='2'><h1 style=\"text-align: center\">\n",
    "Machine Learning </br>\n",
    "</h1>\n",
    "<h2 style=\"text-align: center\">\n",
    "Course Assignment Two </br>\n",
    "</h2>\n",
    "</th>\n",
    "<tr>\n",
    "    <tr>\n",
    "      <th colspan=\"2\">Personal Info</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>First Name:</td>\n",
    "      <td>Ali</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Last Name:</td>\n",
    "      <td>Nikkhah</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Student Number:</td>\n",
    "      <td>99102445</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Github:</td>\n",
    "      <td><a href=\"https://github.com/AliNikkhah2001/MachineLearning01\" target=\"_blank\">https://github.com/AliNikkhah2001/MachineLearning01</a></td>\n",
    "    </tr>\n",
    "  </table>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZR9dmJu-U97"
   },
   "source": [
    "# ****Support Vector Machines****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zL8V07IE-a-K"
   },
   "source": [
    "In this notebook, you will implement SVM for three datasets. You will become more familiar with the Soft-Margin SVM and the Kernel trick, which makes SVM extremely powerful.\n",
    "\n",
    "Before we start our implementation, you must be comfortable with the theoretical details of the Soft-Margin SVM as an optimization problem and be able to derive the dual formulation for this problem. In the next sections, you will solve the dual optimization problem for all datasets using the CVXPY library, which has been developed for solving convex optimization problems. To get more familiar with CVXPY programming, you can use this [link](https://www.cvxpy.org)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XPxlNYRjA2-q"
   },
   "source": [
    "This is the primal formulation for the Soft-Margin SVM for linearly separable data with slack variables $\\xi_i \\ge 0$:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "& {\\text{min.}}\n",
    "& & \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^{N} \\xi_i \\\\\n",
    "& \\text{s.t.}\n",
    "& & y_i(w^T x_i + b) \\geq 1 - \\xi_i, \\; i = 1, \\ldots, N \\\\\n",
    "& & & \\xi_i \\geq 0, \\; i = 1, \\ldots, N\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "Where $w$ represents the weight vector, $b$ is the bias term, and $C$ is the regularization parameter controlling the trade-off between a wide margin and misclassification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OstJ2GITCHPL"
   },
   "source": [
    "**Theoretical Questions:**\n",
    "\n",
    "1) Derive the dual optimization problem for the Soft-Margin SVM.\n",
    "\n",
    "2) How do we calculate the weights and bias ($w$ and $b$) using the dual problem optimum points?\n",
    "\n",
    "3) How do we classify new data points using the optimal weights and bias?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cAWGsmGmDU5L"
   },
   "source": [
    "### Derivation of the Dual Optimization Problem for the Soft-Margin SVM\n",
    "\n",
    "To derive the dual optimization problem, we start with the primal formulation of the Soft-Margin SVM:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\min_{w, b, \\xi} \\quad & \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^{N} \\xi_i \\\\\n",
    "& \\text{s.t.} \\quad & y_i (w^T x_i + b) \\geq 1 - \\xi_i, \\quad i = 1, \\ldots, N \\\\\n",
    "& & \\xi_i \\geq 0, \\quad i = 1, \\ldots, N\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We introduce Lagrange multipliers \\(\\alpha_i \\geq 0\\) for the constraints \\(y_i (w^T x_i + b) \\geq 1 - \\xi_i\\) and \\(\\mu_i \\geq 0\\) for the constraints \\(\\xi_i \\geq 0\\). The Lagrangian function for the primal problem is:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(w, b, \\xi, \\alpha, \\mu) = \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^{N} \\xi_i - \\sum_{i=1}^{N} \\alpha_i \\left( y_i (w^T x_i + b) - 1 + \\xi_i \\right) - \\sum_{i=1}^{N} \\mu_i \\xi_i\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Next, we take the partial derivatives of \\(L\\) with respect to \\(w\\), \\(b\\), and \\(\\xi_i\\), and set them to zero to obtain the conditions for the dual problem:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L}{\\partial w} &= w - \\sum_{i=1}^{N} \\alpha_i y_i x_i = 0 \\quad \\Rightarrow \\quad w = \\sum_{i=1}^{N} \\alpha_i y_i x_i \\\\\n",
    "\\frac{\\partial L}{\\partial b} &= -\\sum_{i=1}^{N} \\alpha_i y_i = 0 \\quad \\Rightarrow \\quad \\sum_{i=1}^{N} \\alpha_i y_i = 0 \\\\\n",
    "\\frac{\\partial L}{\\partial \\xi_i} &= C - \\alpha_i - \\mu_i = 0 \\quad \\Rightarrow \\quad \\alpha_i \\leq C\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Substituting \\(w = \\sum_{i=1}^{N} \\alpha_i y_i x_i\\) into the Lagrangian, we get:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(w, b, \\xi, \\alpha, \\mu) &= \\frac{1}{2} \\left( \\sum_{i=1}^{N} \\alpha_i y_i x_i \\right)^T \\left( \\sum_{i=1}^{N} \\alpha_i y_i x_i \\right) + C \\sum_{i=1}^{N} \\xi_i \\\\\n",
    "& \\quad - \\sum_{i=1}^{N} \\alpha_i \\left( y_i \\left( \\sum_{j=1}^{N} \\alpha_j y_j x_j^T \\right) x_i + b \\right) + \\sum_{i=1}^{N} \\alpha_i (1 - \\xi_i) - \\sum_{i=1}^{N} \\mu_i \\xi_i \\\\\n",
    "&= \\frac{1}{2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_i \\alpha_j y_i y_j x_i^T x_j - \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_i \\alpha_j y_i y_j x_i^T x_j - b \\sum_{i=1}^{N} \\alpha_i y_i + \\sum_{i=1}^{N} \\alpha_i - C \\sum_{i=1}^{N} \\xi_i - \\sum_{i=1}^{N} \\mu_i \\xi_i \\\\\n",
    "&= -\\frac{1}{2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_i \\alpha_j y_i y_j x_i^T x_j + \\sum_{i=1}^{N} \\alpha_i\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The dual problem is then to maximize the dual Lagrangian with respect to \\(\\alpha\\):\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\max_{\\alpha} \\quad & \\sum_{i=1}^{N} \\alpha_i - \\frac{1}{2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_i \\alpha_j y_i y_j x_i^T x_j \\\\\n",
    "& \\text{s.t.} \\quad & 0 \\leq \\alpha_i \\leq C, \\quad i = 1, \\ldots, N \\\\\n",
    "& & \\sum_{i=1}^{N} \\alpha_i y_i = 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### Calculation of Weights and Bias from Dual Problem\n",
    "\n",
    "Once we have found the optimal \\(\\alpha_i\\) from the dual problem, we can calculate the weights \\(w\\) and bias \\(b\\) as follows:\n",
    "\n",
    "1. **Weights**: The weights \\(w\\) are computed using the support vectors and their corresponding \\(\\alpha_i\\):\n",
    "\n",
    "$$\n",
    "w = \\sum_{i=1}^{N} \\alpha_i y_i x_i\n",
    "$$\n",
    "\n",
    "2. **Bias**: To calculate the bias \\(b\\), we use any support vector \\(x_i\\) for which \\(0 < \\alpha_i < C\\):\n",
    "\n",
    "$$\n",
    "b = y_i - w^T x_i = y_i - \\sum_{j=1}^{N} \\alpha_j y_j x_j^T x_i\n",
    "$$\n",
    "\n",
    "### Classification of New Data Points\n",
    "\n",
    "To classify a new data point \\(x\\), we use the sign of the decision function:\n",
    "\n",
    "$$\n",
    "f(x) = w^T x + b\n",
    "$$\n",
    "\n",
    "If \\(f(x) \\geq 0\\), the data point is classified as \\(+1\\); otherwise, it is classified as \\(-1\\).\n",
    "\n",
    "In summary:\n",
    "\n",
    "1. Solve the dual optimization problem to find the optimal \\(\\alpha_i\\).\n",
    "2. Calculate the weights \\(w\\) and bias \\(b\\) using the support vectors.\n",
    "3. Classify new data points using the decision function \\(f(x) = w^T x + b\\)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZWwD5PXLEPdR"
   },
   "source": [
    "Sometimes, the data is not linearly separable. Therefore, the previous formulation of the Soft-Margin SVM does not give a good accuracy for the classification problem. The Kernel trick is a technique used in such situations. Consider $x$ as your input features with dimension $p$. One can use function $\\phi: R^p â†’ R^d$ to map the input features to another space with dimension $d$. Finding good $\\phi$ such that data points become near linearly separable makes the previous formulation of the problem and its dual useful by replacing $\\phi(x_i)$ for $x_i$ in the problem definition. The function $\\phi(x)$ can even be a mapping to an infinite-dimensional space.\n",
    "\n",
    "If you have found the correct dual form of the Soft-Margin SVM in the previous formulation, you can see that the terms $x_i^T x_j$ will appear in the problem formulation and the prediction rule. These terms will be replaced with $\\phi(x_i)^T \\phi(x_j)$ when we use $\\phi$ as our feature mapping function. Given a feature mapping $\\phi$ we define its corresponding **Kernel** to be:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "& K(x, z) = \\phi(x)^T \\phi(z)\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "Hence, in the dual optimization problem and the prediction rule of the Soft-Margin SVM, we can replace all terms in the form $x^T z$ with $K(x, z)$. This technique is called the Kernel trick.\n",
    "\n",
    "Now, given $\\phi$, we could easily compute $K(x,z)$ by finding $\\phi(x)$ and $\\phi(z)$ and taking their inner product. But whatâ€™s more interesting is that often, $K(x,z)$ may be very inexpensive to calculate, even though $\\phi(x)$ itself may be very expensive to calculate (perhaps because it is an extremely high-dimensional vector). Check Stanford's CS229 [notes](https://see.stanford.edu/materials/aimlcs229/cs229-notes3.pdf) on SVM and the Kernel trick for more details and examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AsW6iuw1U_jv"
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ulrV4dEhVCQq"
   },
   "source": [
    "Here are some useful libraries for the implementation. You can add or remove any libraries as you wish. Note that you cannot use sklearn or similar libraries for implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RRB7ZlBqVTEV"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "import cvxpy as cp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJCgsLp4QJtl"
   },
   "source": [
    "## **Dataset Exploration**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4he9swfSQ8E-"
   },
   "source": [
    "In this notebook, you'll be working with three datasets, specifically named \"DF1.csv\", \"DF2.csv\", and \"DF3.csv\". Each dataset entry is composed of a 2-dimensional feature vector and a label, which is either \"1\" or \"-1\". The primary objective of this notebook is to leverage SVM to develop a robust classifier for each dataset.\n",
    "\n",
    "You are given three kernels as follows:\n",
    "\n",
    "\n",
    "\n",
    "1.   Linear Kernel: $K(x, z) = x^T z$\n",
    "2.   Polynomial Kernel of degree 2: $K(x, z) = (1 + âˆ‘_{i = 1}^{p} x_i z_i)^2$, where $p$ is the dimension of the feature space.\n",
    "3.   RBF Kernel: $K(x, z) = e^{-\\frac{||x-z||_2^2}{2Ïƒ^2}}$\n",
    "\n",
    "Before implementing, you must choose the correct kernel for each dataset. Note that each kernel must be used exactly for one dataset. Therefore, you cannot use one of the kernels twice. As a part of our grading scheme, what matters with respect to accuracy is the sum of the accuracies you reach for all three datasets, and the accuracy of each model does not have any separate score. Thus, it would help if you chose the most appropriate kernel for each dataset so that all datasets reach a reasonably good accuracy.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8aHoi0ZUY0o"
   },
   "source": [
    "Load all datasets and show the first 5 rows of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VCFSdyI6REpS"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tlH9BBtdUkQG"
   },
   "source": [
    "Use the following function to visualize the datapoints for all datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZJXxa8mPUjel"
   },
   "outputs": [],
   "source": [
    "def visualize_data(X, y):\n",
    "  col = []\n",
    "  for i in range(len(y)):\n",
    "    if y[i] == 1:\n",
    "      col.append('red')\n",
    "    else:\n",
    "      col.append('blue')\n",
    "  plt.scatter(X[ : , 0], X[ : , 1], color = col)\n",
    "  plt.show()\n",
    "\n",
    "## YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Z4mn4ADVwqJ"
   },
   "source": [
    "**Theoretical Question:**\n",
    "\n",
    "Which kernel will you use for each dataset? Justify your answer.\n",
    "\n",
    "Note: your implementation must follow your choice here. Otherwise, you do not recieve any points for your implemetation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11zXhFE0V9ws"
   },
   "source": [
    "<font color='green'>Write down your answers as markdown here.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5uApej98WtyV"
   },
   "source": [
    "## Dataset one (DF1.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zIhMQnmMWzM9"
   },
   "source": [
    "Split the dataset into train and test sets (20 percent for test). Use 42 as the random state and do not forget to scale the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kKEM5LrQXQh-"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dw1ITF7aXTmP"
   },
   "source": [
    "Use CVXPY to solve the dual problem with the appropriate kernel using the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JJPvPhuyXoKf"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2V4Od6gNXvvf"
   },
   "source": [
    "Predict the class of each entry in the test set using your learned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H-9IAxakX29p"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VqcU4jhBX5Tv"
   },
   "source": [
    "Use classification_report function (from sklearn) to get the accuracy of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2XrEJQ2QYCcR"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QN4wBbNEYXvS"
   },
   "source": [
    "## Dataset two (DF2.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DFxagUuuYXvS"
   },
   "source": [
    "Split the dataset into train and test sets (20 percent for test). Use 42 as the random state and do not forget to scale the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "onEe_NAeYXvS"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yeFzQ7PEYXvT"
   },
   "source": [
    "Use CVXPY to solve the dual problem with the appropriate kernel using the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2uS40Nv2YXvU"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9tgg8gvyYXvU"
   },
   "source": [
    "Predict the class of each entry in the test set using your learned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7QVwa-jlYXvU"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6FEgkFDvYXvV"
   },
   "source": [
    "Use classification_report function (from sklearn) to get the accuracy of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B4XtwtAdYXvV"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8G-gVTXYZ2R"
   },
   "source": [
    "## Dataset three (DF3.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PoXVLKNFYZ2R"
   },
   "source": [
    "Split the dataset into train and test sets (20 percent for test). Use 42 as the random state and do not forget to scale the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BDbYV9dIYZ2S"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nUGJAmMhYZ2S"
   },
   "source": [
    "Use CVXPY to solve the dual problem with the appropriate kernel using the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wFQiz6inYZ2S"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ICETgzlyYZ2T"
   },
   "source": [
    "Predict the class of each entry in the test set using your learned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0juJrHMEYZ2T"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ZGqGA4IYZ2T"
   },
   "source": [
    "Use classification_report function (from sklearn) to get the accuracy of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oCAiDwuNYZ2U"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
